#!/usr/bin/env python


# pip install flatten_json
try:
    from flatten_json import flatten
except ModuleNotFoundError:
    print("\n\nYou need flatten_json: \n\npip install flatten_json\n\n")
    raise
import sys
import csv
import json
from itertools import chain
from pathlib import Path
import argparse


parser = argparse.ArgumentParser(
    description=str(
        "Reads in json (or jsonl) files, flattens it, and emits CSV to stdout. "
        "See flatten_json for details on the flattening."
        "Assumes variation in headers across rows, "
        "so scans them all to create master set of headers before output."
    )
)
parser.add_argument(
    "file_paths", nargs="+", type=Path, help="path to files with json or jsonl"
)
parser.add_argument(
    "--jsonl",
    action="store_true",
    help="file is jsonl (assumed if file is .jsonl)",
)
parser.add_argument(
    "--node",
    help="the node in a dict where resides the list of dicts to become csv",
)

args = parser.parse_args()


def assemble_line_generator(path, args):
    if args.jsonl or path.suffix == ".jsonl":
        return (
            flatten(json.loads(line))
            for line in path.open()
            if line.strip()
        )

    data = json.load(path.open())
    if args.node:
        list_of_dicts = data[args.node]
    else:
        list_of_dicts = data

    return (flatten(mapping) for mapping in list_of_dicts)


data_rows = chain.from_iterable(
    (assemble_line_generator(path, args) for path in args.file_paths)
)


# could throw in an argument to skip deep check of headers
all_keys = set()
replay_rows = []
for row in data_rows:
    all_keys.update(row.keys())
    replay_rows.append(row)


writer = csv.DictWriter(sys.stdout, fieldnames=all_keys)
writer.writeheader()
for data_row in replay_rows:
    writer.writerow(data_row)
