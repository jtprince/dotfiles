#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.10"
# dependencies = [
#     "pandas>=2.2.3",
#     "click>=8.1.7",
#     "azure-identity>=1.24.0",
#     "azure-storage-blob>=12.26.0",
#     "databricks-sql-connector>=3.0.0",
#     "databricks-sdk>=0.40.0",
# ]
# ///
"""
Implementation of download_msrun function without ms-toolkit and
enveda-toolkit dependencies.

This module provides a self-contained way to download mass spectrometry run files
from Azure Blob Storage using Databricks for metadata lookup.
"""

import logging
import pathlib
import warnings
from pathlib import Path
from typing import Literal, Optional, Union

import click
import pandas as pd
from azure.identity import DefaultAzureCredential
from azure.storage.blob import BlobServiceClient
from databricks import sql
from databricks.sdk.oauth import Token

# Configure logging
logger = logging.getLogger(__name__)

# Constants
STORAGE_ACCOUNT = "nassyncncus01"
CONTAINER = "mass-spec"
DATABRICKS_RESOURCE_ID = "2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default"

# Databricks server configurations
DATABRICKS_SERVER = {
    "prod": {
        "server_hostname": "adb-1889469269484926.6.azuredatabricks.net",
        "http_path": "/sql/1.0/warehouses/c2eb77255961a712",
        "small": {"cluster_id": "c2eb77255961a712"},
        "large": {"cluster_id": "c2eb77255961a712"},
    },
    "dev": {
        "server_hostname": "adb-1889469269484926.6.azuredatabricks.net",
        "http_path": "/sql/1.0/warehouses/c2eb77255961a712",
        "small": {"cluster_id": "c2eb77255961a712"},
        "large": {"cluster_id": "c2eb77255961a712"},
    },
}


class Databricks:
    """Databricks client without enveda-toolkit dependencies."""

    def __init__(
        self,
        env: Literal["dev", "prod"],
        compute_size: Literal["small", "large"] = "small",
    ):
        if env not in ["dev", "prod"]:
            raise ValueError(f"Invalid env: {env}")

        self.env = env
        self.compute_size = compute_size

        # Get server configuration
        server_config = DATABRICKS_SERVER[env]
        self.server_hostname = server_config["server_hostname"]
        self.http_path = server_config["http_path"]
        self.cluster_id = server_config[compute_size]["cluster_id"]

    def _get_aad_token(self) -> Token:
        """Get Azure AD JWT token for Databricks authentication."""
        azure_cred = DefaultAzureCredential()
        jwt_token = azure_cred.get_token(DATABRICKS_RESOURCE_ID)

        return Token(
            access_token=jwt_token.token,
            expiry=jwt_token.expires_on,
            token_type="Bearer",
        )

    def _get_databricks_access_token(self):
        """Get Databricks access token."""
        token = self._get_aad_token()
        return token.access_token

    def _get_connection(self, database: str = None):
        """Get a fresh Databricks connection."""
        access_token = self._get_databricks_access_token()
        connection = sql.connect(
            server_hostname=self.server_hostname,
            http_path=self.http_path,
            access_token=access_token,
            auth_type="oauth",
        )

        with connection.cursor() as cursor:
            cursor.execute("USE CATALOG benchling_warehouse")
            if database:
                cursor.execute(f"USE DATABASE {database}")

        return connection

    def query_df(self, query: str, database: str = None) -> pd.DataFrame:
        """Execute a query and return the result as a DataFrame."""
        with self._get_connection(database=database) as connection:
            # Suppress pandas warning about non-SQLAlchemy connections
            with warnings.catch_warnings():
                warnings.filterwarnings("ignore", message=".*SQLAlchemy.*")
                return pd.read_sql_query(query, con=connection)


class AzureBlobStorage:
    """Azure Blob Storage client without enveda-toolkit dependencies."""

    def __init__(self, storage_account: str):
        self.storage_account = storage_account
        self.storage_account_url = f"https://{storage_account}.blob.core.windows.net"

        # Authenticate using DefaultAzureCredential
        self.credentials = DefaultAzureCredential()
        self.blob_service_client = BlobServiceClient(
            account_url=self.storage_account_url, credential=self.credentials
        )

    def download_blob_to_file(
        self, container: str, blob_name: str, local_file_path: Path
    ):
        """Download a blob to a local file."""
        container_client = self.blob_service_client.get_container_client(container)
        blob_client = container_client.get_blob_client(blob_name)

        with open(local_file_path, "wb") as f:
            download_stream = blob_client.download_blob()
            f.write(download_stream.readall())


def download_msrun(
    msrun_id: str,
    env: Literal["dev", "prod"] = "prod",
    local_directory: Optional[Union[Path, str]] = None,
    force_download: bool = False,
) -> pathlib.Path:
    """
    Download msrun function without ms-toolkit and enveda-toolkit dependencies.

    Downloads .d directory for an msrun_id by querying bruker_mass_spec_run table from benchling
    and using cloud_path to download from Azure Blob Storage.

    Parameters
    ----------
    msrun_id : str
        The ID of the mass spec run to download
    env : Literal["dev", "prod"]
        Environment to query data from, by default "prod"
    local_directory : Optional[Union[Path, str]], optional
        Directory where to save the downloaded files, by default None
    force_download : bool, optional
        If True, download files even if they already exist locally, by default False

    Returns
    -------
    Path
        Path to the local directory where the files were downloaded or already exist
    """
    # Determine local directory path
    if not local_directory:
        local_directory = Path(f"./{msrun_id}.d")
    else:
        if str(local_directory).startswith("~"):
            base = Path(local_directory).expanduser()
        else:
            base = Path(local_directory)
        local_directory = base / f"{msrun_id}.d"

    # Define expected files
    msrun_files = ["analysis.tdf", "analysis.tdf_bin"]
    local_files = [local_directory / msrun_file for msrun_file in msrun_files]

    # Check if directory and files already exist
    if not force_download and all(f.exists() for f in local_files):
        logger.info(
            f"The .d folder {local_directory} and all required files already exist. "
            "If you want to force a redownload, set force_download=True."
        )
        return local_directory

    # Continue with database query and download if force=True or files don't exist
    dbx = Databricks(env)
    row = dbx.query_df(
        f"""
        SELECT
            cloud_path
        FROM
            benchling_warehouse.envedatx.bruker_mass_spec_run
        WHERE
            `file_registry_id$` = '{msrun_id}'
        """
    )

    if len(row) == 0:
        raise ValueError(f"No record found for msrun_id: {msrun_id}")
    elif len(row) > 1:
        raise ValueError(f"Multiple records found for msrun_id: {msrun_id}")

    cloud_path = row.to_dict(orient="records")[0]["cloud_path"]
    if cloud_path.startswith("/"):
        cloud_path = cloud_path[1:]
    if cloud_path.endswith("/"):
        cloud_path = cloud_path[:-1]

    logger.info(f"Downloading files to {local_directory}...")
    local_directory.mkdir(exist_ok=True, parents=True)

    # Download files using Azure Blob Storage
    azbs = AzureBlobStorage(storage_account=STORAGE_ACCOUNT)
    container_client = azbs.blob_service_client.get_container_client(CONTAINER)

    for msrun_file, local_path in zip(msrun_files, local_files, strict=False):
        blob_name = f"{cloud_path}/{msrun_file}"
        logger.debug(f"Downloading {blob_name} to {local_directory}...")

        blob_client = container_client.get_blob_client(blob_name)
        with open(local_path, "wb") as f:
            download_stream = blob_client.download_blob()
            f.write(download_stream.readall())

    return local_directory


@click.command()
@click.option(
    "--msrun-id",
    "-m",
    help="Single MSB ID to download (e.g., MSB79001)",
)
@click.option(
    "--file",
    "-f",
    type=click.Path(exists=True),
    help="Path to file with MSB IDs (one per line)",
)
@click.option(
    "--env",
    "-e",
    type=click.Choice(["dev", "prod"]),
    default="prod",
    help="Environment to use (default: prod)",
)
@click.option(
    "--local-directory",
    "-d",
    type=click.Path(),
    help="Directory to save files (default: current directory)",
)
@click.option(
    "--force-download",
    "-F",
    is_flag=True,
    help="Force redownload even if files exist",
)
@click.option(
    "--verbose",
    "-v",
    is_flag=True,
    help="Enable verbose logging",
)
def cli(msrun_id, file, env, local_directory, force_download, verbose):
    """
    Download mass spectrometry run files from Azure Blob Storage.

    Provide either --msrun-id for a single download or --file for batch downloads.

    Examples:

        # Download a single MSB ID
        python download_msrun.py --msrun-id MSB79001

        # Download from a file
        python download_msrun.py --file msrun_ids.txt

        # Download to specific directory
        python download_msrun.py --msrun-id MSB79001 --local-directory ./data

        # Force redownload
        python download_msrun.py --msrun-id MSB79001 --force-download

        # Enable verbose logging
        python download_msrun.py --msrun-id MSB79001 --verbose
    """
    # Configure logging based on verbosity
    if verbose:
        logging.basicConfig(level=logging.INFO)
    else:
        logging.basicConfig(level=logging.WARNING)
        # Only show warnings/errors from azure and databricks libraries
        logging.getLogger("azure").setLevel(logging.WARNING)
        logging.getLogger("databricks").setLevel(logging.WARNING)
        # Keep our logger at INFO to show progress
        logger.setLevel(logging.INFO)

    # Validate inputs
    if not msrun_id and not file:
        raise click.UsageError("Must provide either --msrun-id or --file")
    if msrun_id and file:
        raise click.UsageError("Cannot provide both --msrun-id and --file")

    # Prepare local directory
    local_dir = Path(local_directory) if local_directory else None

    # Collect MSB IDs
    msrun_ids = []
    if msrun_id:
        msrun_ids = [msrun_id]
    elif file:
        with open(file, "r") as f:
            msrun_ids = [line.strip() for line in f if line.strip()]

    # Download each MSB ID
    success_count = 0
    failed_ids = []

    for idx, msb_id in enumerate(msrun_ids, 1):
        try:
            click.echo(f"[{idx}/{len(msrun_ids)}] Processing {msb_id}...")
            result_path = download_msrun(
                msrun_id=msb_id,
                env=env,
                local_directory=local_dir,
                force_download=force_download,
            )
            click.echo(f"  ✓ Downloaded to: {result_path}")
            success_count += 1
        except Exception as e:
            click.echo(f"  ✗ Error downloading {msb_id}: {e}", err=True)
            failed_ids.append(msb_id)

    # Summary
    click.echo("\n" + "=" * 60)
    click.echo(f"Summary: {success_count}/{len(msrun_ids)} successful")
    if failed_ids:
        click.echo(f"Failed IDs: {', '.join(failed_ids)}", err=True)


if __name__ == "__main__":
    cli()
